{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikxoma/.pyenv/versions/3.10.6/lib/python3.10/site-packages/accelerate/utils/imports.py:197: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict, Callable\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == '__main__'\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Nikita and Danik are quite descent researchers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        \n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        \n",
    "        return self.W_E[tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        \n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b97f014f-bf2b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b97f014f-bf2b\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"Nik\", \"ita\", \" and\", \" Dan\", \"ik\", \" are\", \" quite\", \" descent\", \" researchers\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9210671186447144, 0.07893288880586624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30190086364746094, 0.6682272553443909, 0.029871920123696327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6466350555419922, 0.11646536737680435, 0.19996200501918793, 0.03693750873208046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43304404616355896, 0.1216159537434578, 0.08789609372615814, 0.1566726118326187, 0.2007712721824646, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3451928496360779, 0.2126934677362442, 0.10042460262775421, 0.12845738232135773, 0.17280496656894684, 0.04042670875787735, 0.0, 0.0, 0.0, 0.0], [0.5270779728889465, 0.06601674854755402, 0.10844407975673676, 0.02578009106218815, 0.17088012397289276, 0.06585180014371872, 0.03594917058944702, 0.0, 0.0, 0.0], [0.4207213819026947, 0.19521915912628174, 0.05334460735321045, 0.07583615928888321, 0.05513308197259903, 0.055822476744651794, 0.07296308875083923, 0.07096012681722641, 0.0, 0.0], [0.2928483188152313, 0.13702507317066193, 0.04163040965795517, 0.1310722529888153, 0.051020633429288864, 0.07459953427314758, 0.03966737166047096, 0.07817047089338303, 0.15396593511104584, 0.0], [0.28123781085014343, 0.2835495173931122, 0.03683094680309296, 0.04286636784672737, 0.08367685228586197, 0.03430306166410446, 0.02436847798526287, 0.026353659108281136, 0.14006245136260986, 0.046750932931900024]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.1593580893531907e-05, 0.9999884366989136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.3948830857989378e-05, 0.00836145132780075, 0.991614580154419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016167483991011977, 0.0011639200383797288, 0.00030375411733984947, 0.9969155788421631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001811321562854573, 0.007499328814446926, 0.0005447436706162989, 0.00034973988658748567, 0.9914250373840332, 0.0, 0.0, 0.0, 0.0, 0.0], [1.781846185622271e-05, 0.0875573381781578, 0.0008477635565213859, 0.00022787628404330462, 0.0012437287950888276, 0.9101054668426514, 0.0, 0.0, 0.0, 0.0], [0.0006333948113024235, 0.0010241800919175148, 7.42184929549694e-05, 0.0058111632242798805, 0.00022647615696769208, 0.00023965448781382293, 0.991990864276886, 0.0, 0.0, 0.0], [3.288057996542193e-05, 0.00025259717949666083, 1.2827978935092688e-05, 0.00022452170378528535, 6.443847814807668e-05, 4.56838752143085e-05, 3.364354051882401e-05, 0.9993333220481873, 0.0, 0.0], [7.139994704630226e-05, 0.004045234993100166, 7.552810711786151e-05, 0.0005041994154453278, 0.0006089298403821886, 3.468940849415958e-05, 0.0002472617488820106, 0.0004169154854025692, 0.9939957857131958, 0.0], [0.00024415194639004767, 0.007136594504117966, 6.899762956891209e-05, 0.0004878051404375583, 0.0010228169849142432, 0.0002254948194604367, 0.0005079162656329572, 0.0005830356385558844, 0.000929950678255409, 0.9887933135032654]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792288541793823, 0.020771117880940437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9280314445495605, 0.041679054498672485, 0.030289461836218834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5246771574020386, 0.02309034764766693, 0.04442165046930313, 0.4078108072280884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7019132375717163, 0.10727489739656448, 0.06819913536310196, 0.05038480460643768, 0.07222791016101837, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6852335333824158, 0.050214335322380066, 0.09843594580888748, 0.06780225038528442, 0.06926336139440536, 0.029050633311271667, 0.0, 0.0, 0.0, 0.0], [0.5618897080421448, 0.05145532265305519, 0.033418942242860794, 0.11682939529418945, 0.07158643007278442, 0.024189112707972527, 0.14063102006912231, 0.0, 0.0, 0.0], [0.5879338383674622, 0.019236156716942787, 0.07338737696409225, 0.10118676722049713, 0.07309683412313461, 0.02457890287041664, 0.09105365723371506, 0.029526447877287865, 0.0, 0.0], [0.5326313972473145, 0.22500504553318024, 0.05861637741327286, 0.04576984792947769, 0.024460991844534874, 0.015410171821713448, 0.02895677462220192, 0.04689452052116394, 0.022254936397075653, 0.0], [0.5712453126907349, 0.04376111179590225, 0.09546257555484772, 0.033986348658800125, 0.02392720989882946, 0.03289841115474701, 0.037646982818841934, 0.024797169491648674, 0.058725327253341675, 0.07754955440759659]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05384649708867073, 0.946153461933136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013126062229275703, 0.7455885410308838, 0.24128533899784088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16473622620105743, 0.04815179109573364, 0.009988822042942047, 0.7771231532096863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011549534276127815, 0.01372511312365532, 0.0004944344982504845, 0.0008868311415426433, 0.9733440279960632, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002735529269557446, 0.010766693390905857, 0.00023130368208512664, 0.00010807972284965217, 0.0014657092979177833, 0.9871546030044556, 0.0, 0.0, 0.0, 0.0], [0.028520256280899048, 0.002921873005107045, 0.0002268550597364083, 0.010401180014014244, 0.004641527310013771, 0.005665570963174105, 0.9476227760314941, 0.0, 0.0, 0.0], [0.0002658357552718371, 0.0001031713472912088, 1.3479013432515785e-05, 6.598245090572163e-05, 3.611335705500096e-05, 0.000374141673091799, 0.0006757888477295637, 0.9984655380249023, 0.0, 0.0], [0.0040994323790073395, 0.0005723178619518876, 0.000269578886218369, 4.657708268496208e-05, 0.0017408269923180342, 0.00032124854624271393, 0.00033743237145245075, 0.0005731641431339085, 0.9920395612716675, 0.0], [0.007235764525830746, 0.0010664960136637092, 0.00019054647418670356, 6.638695049332455e-05, 0.0045168641954660416, 0.00013859974569641054, 0.00033847722806967795, 0.0033768247812986374, 0.020832285284996033, 0.9622376561164856]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06472159922122955, 0.9352783560752869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02245090715587139, 0.7781036496162415, 0.19944538176059723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.394636332988739, 0.13748908042907715, 0.1177959144115448, 0.35007864236831665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06571973860263824, 0.01993522047996521, 0.0032419245690107346, 0.003943377174437046, 0.9071597456932068, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0076621342450380325, 0.45186296105384827, 0.012612232938408852, 0.005671526771038771, 0.20265920460224152, 0.3195318579673767, 0.0, 0.0, 0.0, 0.0], [0.18682202696800232, 0.05958142876625061, 0.020068712532520294, 0.08928132802248001, 0.14769697189331055, 0.0674276128411293, 0.4291219413280487, 0.0, 0.0, 0.0], [0.022432250902056694, 0.0037724042776972055, 0.0043440465815365314, 0.024175141006708145, 0.009371742606163025, 0.006345536559820175, 0.24738027155399323, 0.6821785569190979, 0.0, 0.0], [0.041251618415117264, 0.01646755449473858, 0.007373907137662172, 0.003037363989278674, 0.0076859090477228165, 0.005931483581662178, 0.003562678350135684, 0.008925564587116241, 0.9057639837265015, 0.0], [0.053320061415433884, 0.0051375944167375565, 0.006527331657707691, 0.006379507947713137, 0.09270396828651428, 0.006402501370757818, 0.01197521761059761, 0.05636567249894142, 0.10263048857450485, 0.6585575938224792]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14669500291347504, 0.853304922580719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012021838687360287, 0.00021780656243208796, 0.9877603650093079, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3659180998802185, 0.003915625158697367, 0.011062322184443474, 0.6191039681434631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08108215034008026, 5.191154923522845e-05, 0.00017646500782575458, 1.3997771020513028e-05, 0.9186755418777466, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006914956495165825, 0.0002241219481220469, 0.00014883225958328694, 9.445334399060812e-07, 1.0468366781424265e-05, 0.9927006959915161, 0.0, 0.0, 0.0, 0.0], [0.08355815708637238, 0.00018709008872974664, 0.001390462159179151, 0.001873982953839004, 0.00010526488040341064, 0.003938708920031786, 0.9089462757110596, 0.0, 0.0, 0.0], [0.00903353188186884, 0.00034172480809502304, 1.2810922271455638e-05, 2.7090272851637565e-05, 2.3125088773667812e-05, 0.00013004049833398312, 1.8630282738740789e-06, 0.9904298186302185, 0.0, 0.0], [0.004871337208896875, 4.1553343180567026e-05, 4.936480308970204e-06, 1.9313723953473527e-07, 2.2807155346526997e-06, 1.5889378346400918e-06, 1.398252607032191e-07, 1.3600603097074782e-06, 0.9950767159461975, 0.0], [0.003616532776504755, 0.0008882922702468932, 8.010913006728515e-06, 3.2284394819726003e-06, 0.0004345797060523182, 3.859533535433002e-05, 2.1335222299967427e-06, 7.6005021583114285e-06, 0.0001973200705833733, 0.9948037266731262]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9397050738334656, 0.060294996947050095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18354782462120056, 0.7452970743179321, 0.07115510106086731, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45096972584724426, 0.4070607125759125, 0.11924884468317032, 0.022720741108059883, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3953950107097626, 0.2633092999458313, 0.09339672327041626, 0.0644189715385437, 0.18348009884357452, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21539394557476044, 0.5726832151412964, 0.07539398968219757, 0.024800848215818405, 0.05269579589366913, 0.05903215333819389, 0.0, 0.0, 0.0, 0.0], [0.28097718954086304, 0.2618887722492218, 0.09537648409605026, 0.026420120149850845, 0.20378448069095612, 0.08927963674068451, 0.042273372411727905, 0.0, 0.0, 0.0], [0.18261408805847168, 0.1531166285276413, 0.04263043776154518, 0.037737976759672165, 0.08688049763441086, 0.04969893395900726, 0.05265824869275093, 0.3946632444858551, 0.0, 0.0], [0.238298699259758, 0.23632079362869263, 0.10999947041273117, 0.05169236660003662, 0.11820685118436813, 0.043951407074928284, 0.020891249179840088, 0.04529288783669472, 0.13534626364707947, 0.0], [0.3556827902793884, 0.15506236255168915, 0.05071759968996048, 0.02386590652167797, 0.039064258337020874, 0.03363294154405594, 0.015245630405843258, 0.033573973923921585, 0.13541863858699799, 0.15773595869541168]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9625491499900818, 0.03745080903172493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3290962874889374, 0.6260460019111633, 0.04485763609409332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3460356295108795, 0.1576465517282486, 0.20149728655815125, 0.29482054710388184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2869870364665985, 0.10539361834526062, 0.107063889503479, 0.37672269344329834, 0.12383269518613815, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12532994151115417, 0.07817518711090088, 0.09401532262563705, 0.16514283418655396, 0.4851492941379547, 0.052187394350767136, 0.0, 0.0, 0.0, 0.0], [0.18534237146377563, 0.0514758862555027, 0.01728222519159317, 0.15471577644348145, 0.1625424176454544, 0.13340330123901367, 0.29523807764053345, 0.0, 0.0, 0.0], [0.13087433576583862, 0.13233768939971924, 0.02002754434943199, 0.11526984721422195, 0.03639741614460945, 0.03385685756802559, 0.3855598568916321, 0.1456764191389084, 0.0, 0.0], [0.1866215020418167, 0.0074393171817064285, 0.018638813868165016, 0.12664085626602173, 0.007307850755751133, 0.017122313380241394, 0.3532120883464813, 0.16633015871047974, 0.11668711155653, 0.0], [0.13581912219524384, 0.014423973858356476, 0.012550854124128819, 0.08772307634353638, 0.02455611526966095, 0.014022957533597946, 0.20153360068798065, 0.14652928709983826, 0.20885470509529114, 0.15398633480072021]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8879808187484741, 0.11201925575733185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8148981332778931, 0.1401255577802658, 0.0449763759970665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24233366549015045, 0.03832035884261131, 0.041895024478435516, 0.6774510145187378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4898678660392761, 0.11550116539001465, 0.08922204375267029, 0.16547338664531708, 0.13993556797504425, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3533780872821808, 0.2536996006965637, 0.10195797681808472, 0.12134572863578796, 0.13547787070274353, 0.03414071723818779, 0.0, 0.0, 0.0, 0.0], [0.14384518563747406, 0.02464871294796467, 0.03258334472775459, 0.35756340622901917, 0.04712854325771332, 0.02753961831331253, 0.3666911721229553, 0.0, 0.0, 0.0], [0.22458839416503906, 0.025762885808944702, 0.04460261017084122, 0.25041672587394714, 0.04586632177233696, 0.04608122259378433, 0.2923937737941742, 0.07028800994157791, 0.0, 0.0], [0.4272739887237549, 0.06937883049249649, 0.07779694348573685, 0.12203910946846008, 0.022647492587566376, 0.049795251339673996, 0.047504864633083344, 0.0845145583152771, 0.09904896467924118, 0.0], [0.42888739705085754, 0.0578538179397583, 0.0542035736143589, 0.12451833486557007, 0.06065686419606209, 0.02281269058585167, 0.08681097626686096, 0.09437989443540573, 0.02929764613509178, 0.040578775107860565]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9870604276657104, 0.012939570471644402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8174858093261719, 0.14344340562820435, 0.03907083347439766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6849213242530823, 0.05915815383195877, 0.08272450417280197, 0.17319609224796295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6623350381851196, 0.08220481872558594, 0.10355336219072342, 0.1311357170343399, 0.020771028473973274, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5492441654205322, 0.10480890423059464, 0.08283641934394836, 0.1404397040605545, 0.08739341050386429, 0.0352773480117321, 0.0, 0.0, 0.0, 0.0], [0.48027467727661133, 0.04888617619872093, 0.07638919353485107, 0.15337717533111572, 0.07326722890138626, 0.046548184007406235, 0.12125744670629501, 0.0, 0.0, 0.0], [0.5048245191574097, 0.04758977144956589, 0.05571002513170242, 0.12206877768039703, 0.07115353643894196, 0.033964235335588455, 0.10235407203435898, 0.06233504042029381, 0.0, 0.0], [0.5214108824729919, 0.07155163586139679, 0.06998909264802933, 0.09838075935840607, 0.04910529777407646, 0.04602327570319176, 0.06817911565303802, 0.06316359341144562, 0.012196309864521027, 0.0], [0.5081334710121155, 0.062247924506664276, 0.0536198616027832, 0.08671729266643524, 0.04229561984539032, 0.030169416218996048, 0.0631331205368042, 0.06340272724628448, 0.06404826790094376, 0.026232236996293068]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7499335408210754, 0.2500664293766022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6514663696289062, 0.08827376365661621, 0.2602598965167999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.690597414970398, 0.055678848177194595, 0.05851861461997032, 0.19520507752895355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4675404131412506, 0.09283585846424103, 0.06336020678281784, 0.12862573564052582, 0.24763792753219604, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39308178424835205, 0.14002305269241333, 0.08247900754213333, 0.10420342534780502, 0.04630967229604721, 0.23390312492847443, 0.0, 0.0, 0.0, 0.0], [0.378905326128006, 0.04447788745164871, 0.05790599808096886, 0.160329669713974, 0.0421358160674572, 0.03656535968184471, 0.27967992424964905, 0.0, 0.0, 0.0], [0.3710286617279053, 0.05044716224074364, 0.03239915147423744, 0.14294776320457458, 0.041671011596918106, 0.026827890425920486, 0.06275075674057007, 0.2719276547431946, 0.0, 0.0], [0.3213804066181183, 0.06025252863764763, 0.045959290117025375, 0.1099652498960495, 0.031850770115852356, 0.024654529988765717, 0.05028097704052925, 0.04941447824239731, 0.30624181032180786, 0.0], [0.3047509789466858, 0.06410081684589386, 0.033968228846788406, 0.09864050894975662, 0.03614044189453125, 0.022807680070400238, 0.058212701231241226, 0.04389224946498871, 0.03308481350541115, 0.30440154671669006]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8246960043907166, 0.17530399560928345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5612883567810059, 0.3846740126609802, 0.05403759703040123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5872053503990173, 0.1608920693397522, 0.029597045853734016, 0.2223055362701416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38262301683425903, 0.4103069007396698, 0.029075991362333298, 0.1316065788269043, 0.04638750106096268, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5249226093292236, 0.2113991677761078, 0.05115678161382675, 0.12783284485340118, 0.05655910447239876, 0.02812938205897808, 0.0, 0.0, 0.0, 0.0], [0.5157448649406433, 0.147617906332016, 0.01999950036406517, 0.1470620334148407, 0.03376910462975502, 0.017210790887475014, 0.11859573423862457, 0.0, 0.0, 0.0], [0.503362774848938, 0.13205888867378235, 0.024730650708079338, 0.10735102742910385, 0.03686950355768204, 0.01769295148551464, 0.059278834611177444, 0.11865532398223877, 0.0, 0.0], [0.33180150389671326, 0.1320129632949829, 0.01505476888269186, 0.11077088117599487, 0.05111062154173851, 0.020288849249482155, 0.07003150135278702, 0.11642321199178696, 0.1525057554244995, 0.0], [0.326110303401947, 0.1606546938419342, 0.01712857000529766, 0.09446480870246887, 0.029143445193767548, 0.014018835499882698, 0.11476818472146988, 0.08614525943994522, 0.07728014141321182, 0.0802856832742691]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x1054601f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "from IPython.display import display\n",
    "\n",
    "html = cv.attention.attention_patterns(\n",
    "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
    "    attention=cache[\"pattern\", 0][0]\n",
    ")\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        \n",
    "        # Calculate query, key and value vectors\n",
    "        q = einops.einsum(\n",
    "            normalized_resid_pre, self.W_Q,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "        ) + self.b_Q\n",
    "        k = einops.einsum(\n",
    "            normalized_resid_pre, self.W_K,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "        ) + self.b_K\n",
    "        v = einops.einsum(\n",
    "            normalized_resid_pre, self.W_V,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "        ) + self.b_V\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q, k,\n",
    "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v, attn_pattern,\n",
    "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = einops.einsum(\n",
    "            z, self.W_O,\n",
    "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
    "        ) + self.b_O\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        \n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        \n",
    "        pre = einops.einsum(\n",
    "            normalized_resid_mid, self.W_in,\n",
    "            \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "        ) + self.b_in\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = einops.einsum(\n",
    "            post, self.W_out,\n",
    "            \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "        ) + self.b_out\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        \n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        \n",
    "        return einops.einsum(\n",
    "            normalized_resid_final, self.W_U,\n",
    "            \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "        ) + self.b_U\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        # SOLUTION\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    debug=False,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=reference_gpt2.cfg.d_vocab\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "\tbatch_size = 16\n",
    "\tepochs = 20\n",
    "\tmax_steps_per_epoch = 200\n",
    "\tlr = 1e-3\n",
    "\tweight_decay = 1e-2\n",
    "\twandb_project: Optional[str] = \"day1-demotransformer\"\n",
    "\twandb_name: Optional[str] = None\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
